# @package _global_

# Note that for ddp2 the effective batch_size must be calculated by the number of gpu nodes.

datamodule:
    _target_: pruneshift.datamodules.datamodule
    name: imagenet100
    root: ${path.imagenet100}
    num_workers: 10
    batch_size: 64 # We need to divide the batch_size by two.

network:
    _target_: pruneshift.networks.network
    network_id: "imagenet100_resnet18"

trainer:
    _target_: pytorch_lightning.Trainer
    max_epochs: 90
    gpus: 4
    accelerator: "ddp"
    benchmark: True
    precision: 16

module:
    _target_: pruneshift.modules.VisionModule
    learning_rate: 0.4

scheduler:
    _target_: pruneshift.modules.MultiStepWarmUpLr
    warmup_end: 5
    milestones: [5, 30, 60, 80]

optimizer:
    _target_: torch.optim.SGD
    momentum: 0.9
    weight_decay: 0.0002
    nesterov: True

seed:
    seed: None
